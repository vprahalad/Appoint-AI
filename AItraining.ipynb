{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/vaishaliprahalad/Library/Python/3.10/lib/python/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install transformers datasets\n",
    "#%pip install tensorflow\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 23ms/step - accuracy: 0.4206 - loss: 1.3144\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.5594 - loss: 0.9681\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.6273 - loss: 0.8479\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.6892 - loss: 0.7337\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7171 - loss: 0.6491\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7543 - loss: 0.5826\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.7890 - loss: 0.5016\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - accuracy: 0.8056 - loss: 0.4592\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.8272 - loss: 0.4098\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.8497 - loss: 0.3722\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8145 - loss: 0.4073\n",
      "Test Accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "# to ensure sentiment analysis is working properly\n",
    "high_urgency_keywords = {\n",
    "    \"burning\": 1,\n",
    "    \"excruciating\": 2, \n",
    "    \"intense\": 1, \n",
    "    \"severe\": 2, \n",
    "    \"chest pain\": 2, \n",
    "    \"difficulty breathing\": 2, \n",
    "    \"high fever\": 2, \n",
    "    \"loss of consciousness\": 3\n",
    "}\n",
    "\n",
    "low_urgency_keywords = {\n",
    "    \"sore throat\": 1, \n",
    "    \"mild cough\": 1, \n",
    "    \"headache\": 1, \n",
    "    \"runny nose\": 1, \n",
    "    \"fatigue\": 1\n",
    "}\n",
    "\n",
    "# extract urgency features based on keywords\n",
    "def keyword_feature_extraction(symptoms):\n",
    "    high_score = 0\n",
    "    low_score = 0\n",
    "    \n",
    "    for word, weight in high_urgency_keywords.items():\n",
    "        if word in symptoms.lower():\n",
    "            high_score += weight\n",
    "    \n",
    "    for word, weight in low_urgency_keywords.items():\n",
    "        if word in symptoms.lower():\n",
    "            low_score += weight\n",
    "    \n",
    "    urgency_score = high_score - low_score\n",
    "\n",
    "    if urgency_score < 1:\n",
    "        urgency_score = 1\n",
    "    elif urgency_score > 5:\n",
    "        urgency_score = 5\n",
    "    \n",
    "    return urgency_score\n",
    "\n",
    "# load training data (10,000 rows)\n",
    "data_train = pd.read_csv('synthetic_patient_data.csv')  \n",
    "descriptions_train = data_train['Description_of_Symptoms']\n",
    "labels_train = data_train['Urgency_Score']\n",
    "\n",
    "# load testing data (500 rows)\n",
    "data_test = pd.read_csv('synthetic_patient_data2.csv')  \n",
    "descriptions_test = data_test['Description_of_Symptoms']\n",
    "labels_test = data_test['Urgency_Score']\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(descriptions_train)\n",
    "sequences = tokenizer.texts_to_sequences(descriptions_train)\n",
    "X = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# apply the keyword extraction function to create a new feature\n",
    "X_keywords = np.array([keyword_feature_extraction(desc) for desc in descriptions_train])\n",
    "\n",
    "# combine text sequences & keyword feature into one input array\n",
    "X_combined = np.hstack((X, X_keywords.reshape(-1, 1)))\n",
    "\n",
    "# encode labels manually\n",
    "unique_labels = np.unique(labels_train)\n",
    "label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "y = np.array([label_to_index[label] for label in labels_train])\n",
    "\n",
    "# build neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 128, input_length=100),\n",
    "    tf.keras.layers.LSTM(64, dropout=0.3, recurrent_dropout=0.2),\n",
    "    tf.keras.layers.Dense(len(unique_labels), activation='softmax')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train model on training dataset\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# tokenize testing data\n",
    "test_sequences = tokenizer.texts_to_sequences(descriptions_test)\n",
    "X_test = pad_sequences(test_sequences, maxlen=100)\n",
    "\n",
    "# encode testing labels\n",
    "y_test = np.array([label_to_index[label] for label in labels_test])\n",
    "\n",
    "# evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Patient Information:\n",
      "Name: Jack\n",
      "Age: 8\n",
      "ID: 1234\n",
      "Symptoms: I have a headache and mild cough.\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "Predicted Urgency Score: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# predict urgency score based on input symptoms\n",
    "def predict_urgency(symptoms, tokenizer, model, label_to_index):\n",
    "    # tokenize and pad input symptoms\n",
    "    sequences = tokenizer.texts_to_sequences([symptoms])\n",
    "    X_input = pad_sequences(sequences, maxlen=100)\n",
    "    \n",
    "    prediction = model.predict(X_input)\n",
    "    \n",
    "    predicted_class_index = np.argmax(prediction, axis=1)[0]\n",
    "    \n",
    "    urgency_score = list(label_to_index.keys())[list(label_to_index.values()).index(predicted_class_index)]\n",
    "    \n",
    "    return urgency_score\n",
    "\n",
    "def main():\n",
    "    patient_name = input(\"Enter your name: \")\n",
    "    patient_age = input(\"Enter your age: \")\n",
    "    patient_id = input(\"Enter your ID: \")\n",
    "    patient_symptoms = input(\"Describe your symptoms: \")\n",
    "    \n",
    "    print(f\"\\nPatient Information:\\nName: {patient_name}\\nAge: {patient_age}\\nID: {patient_id}\\nSymptoms: {patient_symptoms}\\n\")\n",
    "\n",
    "    urgency_score = predict_urgency(patient_symptoms, tokenizer, model, label_to_index)\n",
    "    \n",
    "    print(f\"\\nPredicted Urgency Score: {urgency_score}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
